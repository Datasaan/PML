#Practical Machine Learning - Course Project

#Introduction

#Given data ----accelerometers on the belt, forearm, arm, and dumbell of 6 research study participants.
#Our training data consists of accelerometer data and a label identifying the quality of the activity the participant was doing.
#Our testing data consists of accelerometer data without the identifying label. Our goal is to predict the labels for the test set observations.

#CODE

#Getting and Cleaning Data

library(caret)
train <- read.csv("pml-training.csv")
test <- read.csv("pml-testing.csv")

#Training set is further divided into training set and validation set to calculate the out of sample error

set.seed(10)
inTrain <- createDataPartition(y=train$classe, p=0.7, list=F)
train1 <- train[inTrain, ]
train2 <- train[-inTrain, ]

#Cleaning the data

# remove variables with nearly zero variance
nzv <- nearZeroVar(train1)
train1 <- train1[, -nzv]
train2 <- train2[, -nzv]

# remove variables that are almost always NA
mostlyNA <- sapply(train1, function(x) mean(is.na(x))) > 0.95
train1 <- train1[, mostlyNA==F]
train2 <- train2[, mostlyNA==F]

# The first five variables are useless for predicction

train1 <- train1[, -(1:5)]
train2 <- train2[, -(1:5)]


# Model Building
   #Use Random Forest model. 
   #fit the model on train1.
   # 3-fold CV to select optimal tuning parameters
fitControl <- trainControl(method="cv", number=3, verboseIter=F)

# fit model on train1
fit <- train(classe ~ ., data=ptrain1, method="rf", trControl=fitControl)
## Loading required package: randomForest
## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.
# print final model to see tuning parameters it chose
fit$finalModel
## 
## Call:
##  randomForest(x = x, y = y, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 27
## 
##         OOB estimate of  error rate: 0.23%
## Confusion matrix:
##      A    B    C    D    E class.error
## A 3904    1    0    0    1    0.000512
## B    5 2649    4    0    0    0.003386
## C    0    5 2391    0    0    0.002087
## D    0    0    8 2243    1    0.003996
## E    0    0    0    6 2519    0.00237
# it uses 500 trees and 27 variables at each split.

#Model Evaluation and Selection
# use the fitted model to predict the label (“classe”) in train2.
# use model to predict classe in validation set (train2)
preds <- predict(fit, newdata=train2)

# show confusion matrix to get estimate of out-of-sample error
confusionMatrix(train2$classe, preds)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1674    0    0    0    0
##          B    3 1134    1    1    0
##          C    0    2 1024    0    0
##          D    0    0    2  962    0
##          E    0    0    0    2 1080
## 
## Overall Statistics
##                                         
##                Accuracy : 0.998         
##                  95% CI : (0.997, 0.999)
##     No Information Rate : 0.285         
##     P-Value [Acc > NIR] : <2e-16        
##                                         
##                   Kappa : 0.998         
##  Mcnemar's Test P-Value : NA            
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.998    0.998    0.997    0.997    1.000
## Specificity             1.000    0.999    1.000    1.000    1.000
## Pos Pred Value          1.000    0.996    0.998    0.998    0.998
## Neg Pred Value          0.999    1.000    0.999    0.999    1.000
## Prevalence              0.285    0.193    0.175    0.164    0.184
## Detection Rate          0.284    0.193    0.174    0.163    0.184
## Detection Prevalence    0.284    0.194    0.174    0.164    0.184
## Balanced Accuracy       0.999    0.999    0.998    0.998    1.000
The accuracy is 99.8%, thus my predicted accuracy for the out-of-sample error is 0.2%.

#Re-training the Selected Model
#Before predicting on the test set, it is important to train the model on the full training set (train), rather than using a model trained on a reduced training set (train1), in order to produce the most accurate predictions. Therefore,repeat above on train and test:

# remove variables with nearly zero variance
nzv <- nearZeroVar(train)
train <- train[, -nzv]
test <- test[, -nzv]

# remove variables that are almost always NA
mostlyNA <- sapply(train, function(x) mean(is.na(x))) > 0.95
train <- train[, mostlyNA==F]
test <- test[, mostlyNA==F]

# remove variables 1 to 5
train <- train[, -(1:5)]
test <- test[, -(1:5)]

# re-fit model using full training set (train)
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
fit <- train(classe ~ ., data=train, method="rf", trControl=fitControl)

#Making Test Set Predictions
# predict on test set
preds <- predict(fit, newdata=test)

# convert predictions to character vector
preds <- as.character(preds)

# create function to write predictions to files
pml_write_files <- function(x) {
    n <- length(x)
    for(i in 1:n) {
        filename <- paste0("problem_id_", i, ".txt")
        write.table(x[i], file=filename, quote=F, row.names=F, col.names=F)
    }
}

# create prediction files to submit
pml_write_files(preds)
